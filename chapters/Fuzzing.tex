%!TeX spellcheck = en-US
\documentclass[../main.tex]{subfiles}
\begin{document}

\chapter{Testing and Validation}

\label{chap:fuzzing}

\section{Software testing}

Software testing can be conceptually divided into two main groups: \textbf{Static Testing} and \textbf{Dynamic Testing}. While \textbf{Static Testing} includes only having the code reviewed by a human being, the \textbf{Dynamic Testing} is mainly based on an automated approach. In particular, many different automated tests can be conducted on a single piece of software by an autonomous program. The results of such tests are then used to correct bugs in production (e.g. agile development) or to exploit such bugs if the test is conducted by a malicious counterpart.

In the research I mainly used fuzz testing mixed with other methods such as source code review and reverse engineering. \textbf{Dynamic Testing} can be accomplished by using different approaches such as \textbf{Black Box}, \textbf{White Box} and \textbf{Gray Box}.

\subsection{Black Box}

For this methodology no access to the source code or knowledge of the design
specifications and internal mechanisms of the program is required. The test is
based only on what can be observed, which means to give the target different
inputs and monitor its behavior. This definition reflects exactly the
interaction that a typical user might have with the program. These are exactly
the cases that this kind of testing wants to examine and it should test typical
and atypical user behavior. Black box testing is widely used and it includes all
the techniques where the only available information comes from the user
interaction with the application. One example among all is SQL injection, where
the test is conducted against a plain web page without any previous knowledge of
the logic of the servers and the various scripts. Only interacting with them
allows the user to get an idea of the inner working mechanisms. The test can be
performed directly by the user (manual testing) or using ad hoc programs
(automated testing) such as SQLmap\footnote{http://sqlmap.org/} in this case.
Black Box testing has many advantages: mainly, it does not require access to the
source code so it is always applicable; moreover even in the presence of source
code it can still be effective. Since it is not source code dependent an
effective test case against one program (for example a zip extractor) can be
easily reused to test any programs that implement the same functionality.
However, due to its simplicity this approach can only scratch the surface of the
application that is being tested, achieving an extremely low code coverage. Moreover complex vulnerabilities, which requires multiple attack vectors, will never be triggered and can only be discovered with a White Box or static approach.

\subsection{White Box}

This methodology is nothing more than a source code review or analysis and it
can be done either by a human being or by automated tools. Human analysis in not
always feasible and might be inaccurate, especially in large projects, because
it is easy to miss one line or make simple mistakes. For this reasons automated
source code analysis is the means used in a white box approach. This method
has many advantages mainly regarding code coverage since it is possible to test
all the paths for vulnerabilities. However, source code is rarely available for
projects outside the open source community and this approach still requires a
human interaction to review the results and identify the parts that are actually
related to security problems. Moreover, reviewing a large project, even with the
use of automated tools, might require significant time to complete, effectively
bringing the performance down to the level of the BlackBox approach.

\subsection{Gray Box}

The definition of this technique is quite trivial because several different
methods can be seen as Gray Box approach. This takes the Black Box method as a
starting point augmenting it with the use of common reverse engineering and
binary analysis techniques using disassemblers, debuggers and similar tools. The
majority of these tools still requires human interaction and, reverse
engineering a program can often be a tedious and hard job. However, there are
some automated tools that aim to facilitate this analysis, such as Ropper\footnote{https://github.com/sashs/Ropper} which provides
information about a binary and automatically searches for particular
vulnerabilities(ROP and JOP). For this reasons the Gray Box technique can
efficiently improve the coverage produced by the classic Black Box approach
still without requiring access to the source code. However, reverse
engineering is a complex task and requires specific set of skills and tools
which might not always be available.

\section{The fuzzing world}

\textit{"Fuzzing is the process of sending intentionally invalid data to a
product in the hopes of triggering an error condition or fault."}
\cite{Sutton2007}

Fuzz Testing dates back to 1989 when it started as a project of the operating
system course at the University of Wisconsin-Madison. At his first stages it
was nothing more than a simple black box technique to test the robustness of
some UNIX utilities. The first two fuzzers \textit{fuzz} and \textit{ptyjig},
developed by two students, where incredibly successful in finding previously
unknown vulnerabilities: they where able to crash around 30\% of the considered
utilities using random inputs\cite{firstFuzzPaper}.
It quickly became clear that this was a powerful and strong technique to test
the robustness of a piece of software.

During the years fuzz testing evolved and developed gaining popularity and
expanding to more and more fields (networks, files, wireless and more) until it
became a proper domain of research and engineering. As for today it is
quite vast and, using this kind of tools in the software testing phase would be the best practice. Nevertheless it is still uncommon to find fuzzing
outside the security world, so it is hard to give a precise and unique
definition for this technique. I will try to give an overview of the actual fuzzing world focusing mainly on the binary fuzzing and explaining how the previously mentioned techniques are applied to the fuzzing world.

Fuzzers can be at first divided into two main categories:

\begin{itemize}
  \item{\textbf{Mutation Based (or Dumb)}: it applies random mutations to the
  given  input without any knowledge of the data that are being manipulated.
  Common  mutations include bitflipping, shortening or expanding the input and
  similar.  This  method is a really simple and pure brute force approach and it
  can be  applied  to user provided inputs as well as to automatically generated
  ones to  progressively create a valid input.}

  \item{\textbf{Generation Based (or Smart)}: it requires some knowledge of the
  data or protocol that is being fuzzed and this information is then used to
  generate proper inputs. Such information is either provided by the user
  who defines a specific set of rules or autonomously extracted from a sample
  by the  fuzzer and can be used to generate valid CRC codes for mutated  strings  or  keep the correct  structure for particular files (jpg). Obviously this approach has some  advantages but in some cases a mutation based approach can  give better results  as it can test programs even outside their functioning  boundaries.}

\end{itemize}

Predictably a fuzzer that combines the two methods is the one that can give the most comprehensive results.

Fuzzers can be associated with the previously mentioned testing methods. In this way fuzzers can be further differentiated based on the approach that they have to the problem and the information they require to test the binary. This divides the fuzzing world in three theoretical categories: \textbf{BlackBox Fuzzing}, \textbf{WhiteBox Fuzzing} and \textbf{GrayBox Fuzzing}. \textbf{BlackBox Fuzzing} is the pure black box approach, discussed earlier. It can be both mutation and generation based and it can usually only scratch the surface of a program. \textbf{WhiteBox Fuzzing} requires access to the source code and is a really sophisticated approach which is rarely used since it uses code analysis to generate test cases that will produce full code coverage. For this reason this approach is usually resource and time intensive. \textbf{GrayBox Fuzzing} is halfway between the two and it usually works with or without access to the source code. This method consists in injecting special code during the compilation phase or sandboxing the binary inside an emulator in order to extract information on the execution status and the code coverage achieved.

However, fuzzing is not an exact science and in the real world choosing the right approach can be trivial and usually there is not a clear distinction between the different categories.


\section{State of the art}

There is much software available for many different fuzzing jobs. The choice is vast and goes from the simple hobby project to the much more complex and advanced commercial software. Choosing a good fuzzer is a crucial step for the success of the task: a fuzzer, which is too basic and relays solely on a pure brute force approach (like \textit{ptyjig}) can require ages to find a simple bug on a modern program, whereas a "smarter" fuzzer, will take just a few minutes or hours.

This section is focused on binary fuzzers since the main objective of this research is to test the demodulation/decoding chain. \acrlong{afl} was choosen for the task as it offers many features and it is extensively documented.

\acrlong{afl} and Peach represent the actual state of the art in term of binary fuzzers and are described here.

\subsection{American Fuzzy Lop}

\acrlong{afl} or \textbf{\acrshort{afl}} \footnote{\url{http://lcamtuf.coredump.cx/afl/}} represents the state of the art among binary fuzzers; moreover it has a high rate of vulnerability discovery as stated in the "bug-o-rama trophy case" section of the website. It is widely used and in active development.

\textbf{\acrshort{afl}} has many different features that make it a quite sophisticated software. It uses gray box approach, which require access to the source code in order to perform what it calls "instrumentation". This process consists in adding some custom code to the program that is being compiled; this automatic operation, performed by the custom compilers \textit{afl-gcc} and \textit{afl-clang-fast}, adds small code snippets in critical positions without weighing down the program but allowing the fuzzer to obtain information on the execution flow at run time. It also uses a \textbf{Mutation Based} approach which relies on  some statistical methods to optimize the generation making it efficient and faster to setup.

In addition to instrumentation, \textbf{\acrshort{afl}} can fuzz black box binaries leveraging the QEMU emulator to obtain information about the execution state of the binary and adjust the generation method accordingly. Adopting a black box approach can be useful also when the source code is available since it can highlight vulnerabilities introduced in the compilation/optimization phase. However this can result in reducing the execution speed which considerably slows down the testing.

The information gathered by the fuzzer at execution time will then be used to meaningfully mutate the input. \textbf{AFL} has two different input generation techniques depending on the information supplied: if a file containing a valid input for the program is provided, it is used as a starting point to the next stages otherwise if a blank file is supplied afl starts a "0-day" input generation. This last technique has been proved to be particularly successful, even if more time consuming than the other one. Interestingly \textbf{AFL} was able to generate valid URLs, JPG headers, XML syntax and more~\cite{aflblog}. More specifically the tool uses a genetic algorithm approach which keeps a queue of interesting inputs that will then be mutated. It will append a file to the queue if it triggers a previously unknown internal state. Three different deterministic mutation strategies are applied:
\begin{enumerate}
  \item Sequential bitflips with varying lengths and stepovers: this is a slow but very effective way of mutating the input as it has been observed that with different length of bitflipping it is possible to generate around 130 new paths per million input.
  \item Sequential addition and subtraction of small integers: this method consists in incrementing or decrementing existing integer values in the input file. The operation is performed in 3 different stages: firstly on 8-bit values, then on 16-bit values in both endianess and lastly on 32-bit values. These last two methods are performed only if during the operation the most significant byte is changed; otherwise the value has already been tested in one of the previous cases.
  \item Sequential insertion of known interesting integers: this method uses particular values that are known for their ability to trigger interesting cases, for example -1, MAX\_INT, MAX\_INT-1 and so on. The values are tested in both endianess and in 8,16,32 bits.
\end{enumerate}

There are also non deterministic strategies applied in a never ending loop and which include insertions, deletions, arithmetic, and splicing of different test cases~\cite{afltech}.

Moreover \textbf{AFL} can be parallelized on many cores, in this way a there will be a Master process that coordinates other Slave processes. All the orchestration and synchronization of the processes is handled automatically by \textbf{AFL}. The Master process will also perform all the deterministic mutations while the Slaves will perform the random mutations greatly speeding up the testing. Another usefully characteristic of \textbf{AFL} is that it is also able to resume previously interrupted fuzzing jobs.

One of the best characteristics of \textbf{\acrshort{afl}} is that it is an open source, meaning that every person can improve the code and modify it to meet different needs. For example one really active researcher in the fuzzing field, Dr. Marcel B\"ohme\footnote{\url{https://comp.nus.edu.sg/~mboehme/}}, created some really interesting forks of afl trying to achieve better code coverage\cite{aflfast}\cite{greybf}, and better path discovery\cite{pythia}.

\subsubsection{afl-unicorn}
\label{sub:afl-unicorn}

\textit{afl-unicorn} is a fork of \textbf{\acrshort{afl}}  that was built to leverage the power of \textbf{\acrshort{afl}} to fuzz trivial binaries.

\textit{"For example, maybe you want to fuzz a embedded system that receives input via RF and isn’t easily debugged. Maybe the code you’re interested in is buried deep within a complex, slow program that you can’t easily fuzz through any traditional tools."}\cite{aflunicorn}.

In particular it uses the unicorn engine\cite{unicorn}, which is a CPU emulator, to run the extracted context of a previously running instance of the same program making it architecture independent and easy to selectively fuzz. The unicorn engine allows to fuzz a program in the exact same way as before but, in addition to this, the user has now control over the memory, the CPU registers and the code of the program. This is useful in many different situations, for example when there is no access to the source code or when the sources are available but it is more interesting to fuzz a pre-compiled binary for many different reasons (to test the behavior of a compiler, to test a specific version for a specific architecture and so on). Moreover \textit{afl-unicorn} allows the user to specify portions of code to be fuzzed, i.e. a single function. In addition the fuzzing takes place directly inside the emulated memory manipulating a user defined region. It is also possible to write custom "hooks" when certain functions are called or particular addresses are reached so as to skip specific functions or to trigger a defined behavior, such as a crash, if the program reaches a defined portion of the code.

While it is true that this approach is very powerful it requires a template  to be hand written for the unicorn engine. This requires specific information like the heap addresses, the register values and the content as well as the addresses of all the memory regions. The extraction of such information is a trivial process since a binary must be running inside a debugger and, usually, the relevant portion of code must be identified. Only then the template can be written. This process is time consuming and requires very specific knowledge of the hardware. As a matter of fact the context of the process must be dumped from the memory and the binary must be disassembled to find the boundaries inside which the emulation must take place.

For those tasks some tools like \textit{unicorn\_dumper.py} and a basic template are provided with the tool; other programs like \textit{unicorn\_template\_generator.py} and \textit{extract\_from\_memory.py} have been  written specifically for this task and will be illustrated in Chapter \ref{chap:experimsetup}.

\subsection{Peach}

\textbf{Peach} is another very popular fuzzer and is the commercial counterpart of \textbf{AFL}. However, the two programs are different since \textbf{Peach} is capable of fuzzing different targets such as network protocols, device drivers, file consumers, embedded devices, external devices natively, while in \textbf{AFL} all these capabilities are added by specific forks. Another difference comes in the generation of inputs. As a matter of fact \textbf{Peach} is a \textbf{BlackBox} \textbf{Generation Based} fuzzer that requires no access to the source code but a \textit{"Peach Pit"} must be supplied. A \textit{Pit} is a template, defined by the user, which the fuzzer uses to generate the input data, monitor the target and display the status and results. It contains the following information:

\begin{itemize}
  \item A description of the data layout to test.
  \item The agent, monitors and I/O adapter to use in the fuzzing session.
  \item Setup parameters, such as Port or Log Folder, to use in the fuzzing session.
\end{itemize}

As an added factor \textit{Pits} are open source and shared inside the \textbf{Peach} community which makes finding the perfect \textit{Peach Pit} most likely. Moreover from the Whitepaper it is possible to understand that there are \textit{"over 50 mutation algorithms"} but there is no explanation on what these algorithms are doing or how~\cite{peach}.

Peach was open source but from version 3 the policy changed as it is stated on the website: \textit{"Peach is not open source software, it’s FREE software. Peach is licensed under the MIT License which places no limitations on it’s use. This software license was selected to guarantee that companies and individuals do not have to worry about license tainting issues."}~\cite{peachlicense}. This license is ambiguous because on the company website a Community Edition of \textbf{Peach 3} is available and it is stated to be open source (source code is also available). This is probably due to the fact that the company sells fuzzing services using \textbf{Peach} so the commercial version might have more features. However an in development fork of \textbf{Peach 2.7} is maintained by Mozilla.\footnote{https://github.com/MozillaSecurity/peach}

\end{document}
